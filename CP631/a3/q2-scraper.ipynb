{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "import sys\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "\n",
    "# The base url that is going to be scraped. Leaving the end of the url open so it can be concated later\n",
    "base = \"https://datacrunch.9c9media.ca/statsapi/sports/hockey/leagues/nhl/sortablePlayerSeasonStats/skater?brand=tsn&type=json&seasonType=regularSeason&season=\"\n",
    "\n",
    "# Grab page data and JSON parse it\n",
    "def retrievePage(year):\n",
    "    try:\n",
    "        doc = request.urlopen(base + year)\n",
    "    except HTTPError as e:\n",
    "        return\n",
    "    return json.loads(str(bs(doc.read(), \"html.parser\")))\n",
    "\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "# For each year of NHL stats, lets scrape.\n",
    "for year in [\"2018\", \"2019\", \"2020\", \"2021\"]:\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        # grab page of JSON data\n",
    "        stats = executor.submit(retrievePage, year)\n",
    "        print('Retrieve NHL stats for year {}'.format(year))\n",
    "        pages = []\n",
    "        # if page grab is complete append to pages array\n",
    "        if (concurrent.futures.as_completed(stats)):\n",
    "            try:\n",
    "                pages.append((year, stats.result()))\n",
    "            except:\n",
    "                pages.append((year, None))\n",
    "    # write out the stat page grabed for iterated year\n",
    "    with open('stats_page_{}.json'.format(year), 'w') as file_:\n",
    "        file_.writelines(\"\\n\".join(json.dumps((year, page_i[1])) for page_i in pages))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
